# Azure OpenAI Deployment Configuration
# Optimized for performance and cost efficiency

deployments:
  gpt-4:
    deployment_name: ${AZURE_GPT4_DEPLOYMENT:gpt-4}
    api_version: "2024-02-15-preview"
    
    # Rate limits based on Azure tier
    rate_limits:
      requests_per_minute: 60
      tokens_per_minute: 90000
      
    # Performance settings
    performance:
      max_concurrent_requests: 5
      timeout_seconds: 60
      retry_attempts: 3
      retry_backoff_base: 2
      
    # Request optimization
    optimization:
      batch_size: 5
      cache_ttl_seconds: 3600
      use_streaming: false
      
    # Cost optimization
    cost:
      max_tokens_default: 2000
      temperature_default: 0.3
      
  gpt-4-turbo:
    deployment_name: ${AZURE_GPT4_DEPLOYMENT:gpt-4}
    api_version: "2024-02-15-preview"
    
    rate_limits:
      requests_per_minute: 100
      tokens_per_minute: 150000
      
    performance:
      max_concurrent_requests: 8
      timeout_seconds: 45
      retry_attempts: 3
      retry_backoff_base: 2
      
    optimization:
      batch_size: 10
      cache_ttl_seconds: 3600
      use_streaming: true
      
    cost:
      max_tokens_default: 2000
      temperature_default: 0.3
      
  gpt-35-turbo:
    deployment_name: ${AZURE_GPT35_DEPLOYMENT:gpt-35-turbo}
    api_version: "2024-02-15-preview"
    
    rate_limits:
      requests_per_minute: 300
      tokens_per_minute: 250000
      
    performance:
      max_concurrent_requests: 20
      timeout_seconds: 30
      retry_attempts: 3
      retry_backoff_base: 1.5
      
    optimization:
      batch_size: 20
      cache_ttl_seconds: 7200
      use_streaming: true
      
    cost:
      max_tokens_default: 1000
      temperature_default: 0.3

# Cache configuration
cache:
  provider: redis
  redis:
    host: ${REDIS_HOST:localhost}
    port: ${REDIS_PORT:6379}
    db: ${REDIS_DB:1}
    password: ${REDIS_PASSWORD:}
    
  strategies:
    # Cache deterministic responses (temperature <= 0.3)
    deterministic:
      enabled: true
      ttl_seconds: 3600
      
    # Cache embedding responses
    embeddings:
      enabled: true
      ttl_seconds: 86400
      
    # Don't cache creative responses (temperature > 0.7)
    creative:
      enabled: false

# Monitoring and alerting
monitoring:
  metrics:
    - name: request_latency
      type: histogram
      buckets: [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
      
    - name: token_usage
      type: counter
      labels: [model, deployment]
      
    - name: error_rate
      type: gauge
      threshold: 0.05
      
  alerts:
    - name: high_error_rate
      condition: error_rate > 0.10
      severity: critical
      
    - name: rate_limit_approaching
      condition: requests_per_minute > 0.8 * limit
      severity: warning
      
    - name: high_latency
      condition: p99_latency > 5.0
      severity: warning

# Load balancing strategy
load_balancing:
  strategy: weighted_round_robin
  health_check_interval: 30
  
  # Weights for different use cases
  weights:
    gpt-4:
      complex_tasks: 1.0
      simple_tasks: 0.1
      
    gpt-4-turbo:
      complex_tasks: 0.8
      simple_tasks: 0.3
      
    gpt-35-turbo:
      complex_tasks: 0.2
      simple_tasks: 1.0